{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c35598b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13350\n"
     ]
    }
   ],
   "source": [
    "print(\"13350\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207639a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m      2\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m      3\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt') \n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet') \n",
    "nltk.download('averaged_perceptron_tagger') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4c8e6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Welcome\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Welcome\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Welcome\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Welcome\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')  \n",
    "nltk.download('averaged_perceptron_tagger')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a775327f",
   "metadata": {},
   "outputs": [],
   "source": [
    " text= \"Stemming is a normalization technique where lists of tokenized words are converted into shortened root words to remove redundancy. Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fc16e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stemming is a normalization technique where lists of tokenized words are converted into shortened root words to remove redundancy.', 'Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize \n",
    "tokenized_text= sent_tokenize(text) \n",
    "print(tokenized_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daba0902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stemming', 'is', 'a', 'normalization', 'technique', 'where', 'lists', 'of', 'tokenized', 'words', 'are', 'converted', 'into', 'shortened', 'root', 'words', 'to', 'remove', 'redundancy', '.', 'Stemming', 'is', 'the', 'process', 'of', 'reducing', 'inflected', '(', 'or', 'sometimes', 'derived', ')', 'words', 'to', 'their', 'word', 'stem', ',', 'base', 'or', 'root', 'form', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "tokenized_word=word_tokenize(text) \n",
    "print(tokenized_word) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d373e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i', 'mustn', 'what', 'from', 'needn', \"doesn't\", 'are', 'in', \"mustn't\", 'before', 'they', 'his', \"it's\", 'don', 'should', \"shan't\", 'such', 'yourself', 'more', \"they're\", 'hadn', 'here', 'where', \"that'll\", 'be', 'over', 'but', 'a', 'or', 'with', \"it'd\", 'himself', 'me', 'ourselves', 'on', 'we', \"should've\", 'who', \"you've\", 'being', 'few', 'hasn', 'own', \"couldn't\", 'o', \"he's\", 'y', 'herself', 'some', 'these', 'it', \"wouldn't\", \"i'm\", \"don't\", 'd', 'against', \"she'd\", 'below', 'for', 'through', \"hadn't\", 'too', 'weren', \"haven't\", \"needn't\", 'll', \"won't\", \"it'll\", \"you'll\", \"he'd\", 'hers', 'wouldn', 'whom', 'didn', 're', 'yours', 'nor', 'their', 'themselves', 'her', 'wasn', \"you're\", 'couldn', 'had', 'into', 'very', \"hasn't\", 'as', 'my', 'than', 'about', 'because', 'doesn', 'and', 'down', 'same', 'once', \"aren't\", 'that', 'haven', 'them', 'during', 'theirs', 'any', 'he', 'shouldn', \"we're\", 't', 'him', \"we'll\", \"we'd\", 'again', 'was', 'further', 'not', 'only', 's', 'those', 'will', 'm', \"she's\", 've', 'both', 'she', 'were', 'your', \"i've\", 'our', 'am', 'out', 'when', \"we've\", \"they'll\", 'while', 'of', 'just', 'ours', 'doing', 'to', 'by', 'shan', 'do', 'isn', 'having', \"mightn't\", 'its', 'most', 'under', 'an', 'which', \"didn't\", 'can', 'how', 'aren', 'there', \"isn't\", 'between', 'after', 'has', \"i'd\", \"wasn't\", 'other', 'why', 'off', 'ain', 'each', 'so', \"weren't\", 'up', 'above', 'does', \"i'll\", 'been', 'itself', \"shouldn't\", \"they've\", 'this', 'you', 'mightn', \"you'd\", 'then', 'did', \"she'll\", 'the', \"he'll\", 'until', 'no', 'won', 'is', 'all', 'have', 'if', \"they'd\", 'at', 'now', 'myself', 'yourselves', 'ma'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stop_words=set(stopwords.words(\"english\")) \n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97f7b284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['stopwords', 'considered', 'as', 'noise', 'in', 'the', 'text', 'text', 'may', 'contain', 'stop', 'words', 'such', 'as', 'is', 'am', 'are', 'this', 'a', 'an', 'the', 'etc']\n",
      "Filterd Sentence: ['stopwords', 'considered', 'noise', 'text', 'text', 'may', 'contain', 'stop', 'words', 'etc']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text= \"Stopwords considered as noise in the text. Text may contain stop words such as is, am, are, this, a, an, the, etc.\"\n",
    "text= re.sub('[^a-zA-Z]', ' ',text) \n",
    "tokens = word_tokenize(text.lower()) \n",
    "filtered_text=[] \n",
    "for w in tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_text.append(w) \n",
    "print(\"Tokenized Sentence:\",tokens) \n",
    "print(\"Filterd Sentence:\",filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd71224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
